### NLP with Attention Models

### Neural Machine Translation (Week 1)

Discover some of the shortcomings of a traditional seq2seq model and how to solve for them by adding an attention mechanism, then build a Neural Machine Translation model with Attention that translates English sentences into German.

### Learning Objectives

* Explain how an Encoder/Decoder model works
* Apply word alignment for machine translation
* Train a Neural Machine Translation model with Attention
* Develop intuition for how teacher forcing helps a translation model checks its predictions
* Use BLEU score and ROUGE score to evaluate machine-generated text quality
* Describe several decoding methods including MBR and Beam search

### Text Summarization (Week 2)

Compare RNNs and other sequential models to the more modern Transformer architecture, then create a tool that generates text summaries.

### Learning Objectives

* Describe the three basic types of attention
* Name the two types of layers in a Transformer
* Define three main matrices in attention
* Interpret the math behind scaled dot product attention, causal attention, and multi-head attention
* Use articles and their summaries to create input features for training a text summarizer
* Build a Transformer decoder model (GPT-2)

### Question Answering

### Learning Objectives

Gain intuition for how transfer learning works in the context of NLP
Identify two approaches to transfer learning
Discuss the evolution of language models from CBOW to T5 and Bert
Fine-tune BERT on a dataset
Implement context-based question answering with T5
Interpret the GLUE benchmark

### Chatbot

### Learning Objectives

* Explain the motivation for reversible layers
* Integrate locality sensitive hashing into attention layers
* Describe the Reformer model
