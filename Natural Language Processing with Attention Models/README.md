### NLP with Attention Models

#### Neural Machine Translation (Week 1)

Discover some of the shortcomings of a traditional seq2seq model and how to solve for them by adding an attention mechanism, then build a Neural Machine Translation model with Attention that translates English sentences into German.

## Learning Objectives

* Explain how an Encoder/Decoder model works
* Apply word alignment for machine translation
* Train a Neural Machine Translation model with Attention
* Develop intuition for how teacher forcing helps a translation model checks its predictions
* Use BLEU score and ROUGE score to evaluate machine-generated text quality
* Describe several decoding methods including MBR and Beam search

#### Text Summarization (Week 2)

Compare RNNs and other sequential models to the more modern Transformer architecture, then create a tool that generates text summaries.
