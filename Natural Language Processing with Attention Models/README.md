### NLP with Attention Models

#### Neural Machine Translation (Week 1)

Discover some of the shortcomings of a traditional seq2seq model and how to solve for them by adding an attention mechanism, then build a Neural Machine Translation model with Attention that translates English sentences into German.

#### Text Summarization (Week 2)

Compare RNNs and other sequential models to the more modern Transformer architecture, then create a tool that generates text summaries.
