# NLP with Classification and Vector Spaces

## Week 1: 

### Sentiment Analysis with Logistic Regression

Learn to extract features from text into numerical vectors, then build a binary classifier for tweets using a logistic regression!

#### Learning Objectives

* Sentiment analysis
* Logistic regression
* [Data pre-processing](Natural%20Language%20Processing%20with%20Classification%20and%20Vector%20Spaces/Week%201/NLP_C1_W1_lecture_nb_01.ipynb)
* [Calculating word frequencies](Natural%20Language%20Processing%20with%20Classification%20and%20Vector%20Spaces/Week%201/NLP_C1_W1_lecture_nb_02.ipynb)
* Feature extraction
* Vocabulary creation
* Supervised learning
* [Visualising Tweets](Natural%20Language%20Processing%20with%20Classification%20and%20Vector%20Spaces/Week%201/NLP_C1_W1_lecture_nb_03.ipynb)

#### Assignment 1:

[Sentiment Analysis with Logistic Regression](Natural%20Language%20Processing%20with%20Classification%20and%20Vector%20Spaces/Week%201/C1_W1_Assignment.ipynb)

## Week 2

### Sentiment Analysis with Naïve Bayes

Learn the theory behind Bayes' rule for conditional probabilities, then apply it toward building a Naive Bayes tweet classifier of your own!

#### Learning Objectives

* Error analysis
* Naive Bayes inference
* Log likelihood
* Laplacian smoothing
* conditional probabilities
* Bayes rule
* Sentiment analysis
* Vocabulary creation
* Supervised learning

#### Assignment 2:

[Sentiment Analysis with Naïve Bayes](https://github.com/vigneshv92/Natural-Language-Specialization/blob/master/Natural%20Language%20Processing%20with%20Classification%20and%20Vector%20Spaces/Week3/C1_W3_Assignment.ipynb)

## Week 3

### Vector Space Models

Vector space models capture semantic meaning and relationships between words. You'll learn how to create word vectors that capture dependencies between words, then visualize their relationships in two dimensions using PCA.

#### Learning Objectives

* Covariance matrices
* Dimensionality reduction
* Principal component analysis
* Cosine similarity
* Euclidean distance
* Co-occurrence matrices
* Vector representations
* Vector space models

## Week 4

#### Learning Objectives

* Gradient descent
* Approximate nearest neighbors
* Locality sensitive hashing
* Hash functions
* Hash tables
* K nearest neighbors
* Document search
* Machine translation
* Frobenius norm

#### Labs

* [Rotation Matrices in R2 / Hash Tables](https://github.com/vigneshv92/Natural-Language-Specialization/blob/master/Natural%20Language%20Processing%20with%20Classification%20and%20Vector%20Spaces/Week4/NLP_C1_W4_lecture_nb_01.ipynb)
